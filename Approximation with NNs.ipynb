{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Approximation with neural networks\n",
    "\n",
    "\n",
    "Goal for the day: Building a playground for testing the approximation properties of neural networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warmup: \n",
    "\n",
    "Go to \n",
    "https://playground.tensorflow.org/\n",
    "\n",
    "and train a neural net to 0 training loss on the four type of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ.pop(\"MPLBACKEND\", None)\n",
    "os.environ.pop(\"QT_OPENGL\", None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backend: qtagg\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "print(\"Backend:\", matplotlib.get_backend())\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot([1,2,3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.  Define our network\n",
    "\n",
    "Question: \n",
    "Write two fully connected networks, with dims $1, h, 1$ and $1, h, h, h, h, 1$, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPshallow(nn.Module):\n",
    "    def __init__(self, hidden_dim=10, input_dim=1):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.net_type = \"shallow\"  # for keeping info\n",
    "        self.l1 = nn.Linear(input_dim, self.hidden_dim)\n",
    "        self.lout = nn.Linear(self.hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = self.lout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MLPdeep(nn.Module):\n",
    "    def __init__(self, hidden_dim=10, input_dim=1):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.net_type = \"deep\"  # for keeping info\n",
    "        self.l1 = nn.Linear(input_dim, self.hidden_dim)\n",
    "        self.l2 = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        self.l3 = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        self.l4 = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        self.l5 = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        self.lout = nn.Linear(self.hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = F.relu(self.l3(x))\n",
    "        x = F.relu(self.l4(x))\n",
    "        x = F.relu(self.l5(x))\n",
    "        x = self.lout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective: Train a neural net to approximate some arbitrary functions\n",
    "\n",
    "Here we define the function we are going to try to approximate. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_f(x):\n",
    "    return np.maximum(0.3 * x, 0)\n",
    "\n",
    "\n",
    "def middle_f(x):\n",
    "    return np.abs(x + 0.5) - 2 * np.maximum(x - 0.5, 0) - 0.4\n",
    "\n",
    "\n",
    "def complex_f(x):\n",
    "    return np.sin(10 * x) * np.cos(2 * x + 1)\n",
    "\n",
    "\n",
    "def gauss_f(x, d=1, x0=0, sigma=8):\n",
    "    return np.exp(-np.dot((x - x0), (x - x0)) / 2 * (sigma**2))\n",
    "\n",
    "\n",
    "xs = np.linspace(-1, 1, 100)\n",
    "plt.plot(xs, [simple_f(x) for x in xs], label=\"Simple f\")\n",
    "plt.plot(xs, [middle_f(x) for x in xs], label=\"Middle f\")\n",
    "plt.plot(xs, [complex_f(x) for x in xs], label=\"Complicated f\")\n",
    "# plt.plot(xs, [gauss_f(x) for x in xs], label=\"Gauss f\")\n",
    "plt.title(\"Target functions\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "    \"\"\"\n",
    "    Generates batches of (labels, responses) pairs, of the form (x_i,  f(x_i) + noise).\n",
    "    x_i are 1-dimensional\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, n=1000, xmin=-1, xmax=1, noise_level=1e-2, type=\"simple\", input_dim=1\n",
    "    ):\n",
    "        self.n = n  # number of data points\n",
    "        self.xmin = xmin  # min feature\n",
    "        self.xmax = xmax  # max feature\n",
    "\n",
    "        self.noise_level = noise_level  # gaussian noise of variance noise_level**2\n",
    "        self.type = type  # define the target function\n",
    "\n",
    "        self.inputs = torch.empty(n, input_dim)  # all inputs in our dataset\n",
    "        self.outputs = torch.empty(n, 1)  # all responses\n",
    "\n",
    "        self.d = input_dim\n",
    "\n",
    "        self.fill_data()  # fill inputs and outputs\n",
    "\n",
    "        self.pass_order = np.arange(\n",
    "            n\n",
    "        )  # will be shuffled every time we go through the data\n",
    "        self.current_position = (\n",
    "            0  # current position in pass order. Used to generate batches.\n",
    "        )\n",
    "\n",
    "    def true_f(self, x):\n",
    "        if self.type == \"simple\":\n",
    "            return simple_f(x)\n",
    "        if self.type == \"middle\":\n",
    "            return middle_f(x)\n",
    "        if self.type == \"complex\":\n",
    "            return complex_f(x)\n",
    "        if self.type == \"gauss\":\n",
    "            x0 = 0.1\n",
    "            return gauss_f(x, d=self.d, x0=x0)\n",
    "\n",
    "    def next_batch(self, batch_size=10):\n",
    "        pos = self.current_position\n",
    "        self.current_position = (self.current_position + batch_size) % self.n\n",
    "\n",
    "        indices = self.pass_order[pos : pos + batch_size]\n",
    "        input_batch = torch.stack([self.inputs[i] for i in indices])\n",
    "        output_batch = torch.stack([self.outputs[i] for i in indices])\n",
    "\n",
    "        if pos + batch_size > self.n:\n",
    "            np.random.shuffle(self.pass_order)\n",
    "\n",
    "        return input_batch, output_batch\n",
    "\n",
    "    def fill_data(self):\n",
    "        for i in range(self.n):\n",
    "            x = self.xmin + torch.rand(self.d) * (self.xmax - self.xmin)\n",
    "            y = self.true_f(x)\n",
    "            self.inputs[i] = x\n",
    "            self.outputs[i] = y + self.noise_level * np.random.normal() * (\n",
    "                np.random.rand() < 0.15\n",
    "            )\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Data(50, type=\"middle\", noise_level=1)\n",
    "plt.title(\"Target function and available data\")\n",
    "plt.plot(xs, dataset.true_f(xs), linestyle=\"--\", alpha=0.2, label=\"True values\")\n",
    "plt.scatter(dataset.inputs, dataset.outputs, marker=\"x\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train a network\n",
    "\n",
    "In this section we train a network to match the target function, given the observed points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 100\n",
    "mlp = MLPdeep(hidden_dim=hidden_dim)  # Initialize a network\n",
    "\n",
    "total_train_steps = 0  # for log keeping\n",
    "all_losses = []  # for log keeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(net, dataset, n_points=1000):\n",
    "    with torch.no_grad():\n",
    "        xs = torch.linspace(-1, 1, n_points).reshape(n_points, 1)\n",
    "        nn_values = net(xs)\n",
    "        true_vals = dataset.true_f(xs)\n",
    "        plt.plot(xs, true_vals, linestyle=\"--\", alpha=0.2, label=\"True values\")\n",
    "        plt.plot(xs, nn_values, label=\"Current approx\")\n",
    "        plt.scatter(dataset.inputs, dataset.outputs, marker=\"x\", label=\"data\")\n",
    "\n",
    "        print(f\"Squared error of O pred: {np.linalg.norm(true_vals)**2 / n_points}\")\n",
    "        print(\n",
    "            f\"Squared error of mlp: {np.linalg.norm(nn_values - true_vals)**2 / n_points}\"\n",
    "        )\n",
    "        print(f\"total_train_steps : {total_train_steps}\")\n",
    "\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Squared error of O pred: 0.0932499235305304\n",
      "Squared error of mlp: 0.0963609142250798\n",
      "total_train_steps : 0\n"
     ]
    }
   ],
   "source": [
    "plot(mlp, dataset)\n",
    "plt.title(\"At initialization\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "We are going to try to compute an approximate least-squares neural network, by minimizing\n",
    "$$\n",
    "    \\frac{1}{n} \\sum_{ \\text{data}} (h_w(x_i) - y_i)^2\n",
    "$$\n",
    "To do so, we use the standard neural net training procedure: (stochastic) gradient descent on then weights of the neural network. \n",
    "$$\n",
    "    w_{t+1} = w_t - \\mathrm{lr}  \\cdot \\nabla_w f(w_t; (x_i, y_i))\n",
    "$$\n",
    "where\n",
    "$$\n",
    "    f(w; (x_i, y_i) ) = \\frac{1}{n} \\sum_{ \\text{data}} (h_w(x_i) - y_i)^2 \\, . \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the optimizer and loss\n",
    "\n",
    "You can come back and tune the learning rate if necessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(mlp.parameters(), lr=0.0001)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the training procedure. Go to TP1 if you need a reminder on the syntax. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training steps : 12000000\n"
     ]
    }
   ],
   "source": [
    "N_steps = 100_000\n",
    "batch_size = 30\n",
    "\n",
    "losses = []\n",
    "\n",
    "for i in range(N_steps):\n",
    "    inputs, labels = dataset.next_batch(batch_size)\n",
    "    # print(inputs.shape)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward + backward + optimize\n",
    "    outputs = mlp(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    loss.backward()\n",
    "    losses.append(loss.detach().numpy())\n",
    "    optimizer.step()\n",
    "    total_train_steps += batch_size\n",
    "\n",
    "all_losses += losses\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "print(f\"Total training steps : {total_train_steps}\")\n",
    "axs[0].set_title(\"All losses\")\n",
    "axs[0].plot(all_losses)\n",
    "axs[1].set_title(\"Losses in latest training steps\")\n",
    "axs[1].plot(losses)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Squared error of O pred: 0.0932499235305304\n",
      "Squared error of mlp: 0.009954492699966977\n",
      "total_train_steps : 9000000\n"
     ]
    }
   ],
   "source": [
    "plot(mlp, dataset)\n",
    "plt.title(\"After training\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train your network until the plot looks good. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Compare the performances on different functions for different depths.\n",
    "\n",
    "\n",
    "Plot the approximation after $N \\approx 1\\,000\\,000$ steps for the two architectures (with, e.g., hidden_dim = 15) on the three functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, dataset, N_steps, batch_size, lr):\n",
    "    optimizer = optim.SGD(net.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    losses = []\n",
    "\n",
    "    for _ in range(N_steps):\n",
    "        inputs, labels = dataset.next_batch(batch_size)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        losses.append(torch.mean(loss.detach()))\n",
    "\n",
    "        optimizer.step()\n",
    "    return losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_points = 10_000\n",
    "\n",
    "all_data_types = [\"simple\", \"middle\", \"complex\"]\n",
    "all_net_types = [\"shallow\", \"deep\"]\n",
    "\n",
    "N_steps = 100_000\n",
    "batch_size = 10\n",
    "lr = 0.01\n",
    "\n",
    "fig, axs = plt.subplots(2, 3, figsize=(20, 10))\n",
    "\n",
    "for i, type in enumerate(all_data_types):\n",
    "    dataset = Data(30, type=type)\n",
    "\n",
    "    net_shallow = MLPshallow(hidden_dim=20)\n",
    "    train(net_shallow, dataset, N_steps, batch_size, lr)\n",
    "\n",
    "    xs = torch.linspace(-1, 1, n_points).reshape(n_points, 1)\n",
    "    nn_values = net_shallow(xs).detach().numpy()\n",
    "    true_vals = dataset.true_f(xs)\n",
    "    axs[0, i].plot(xs, true_vals, linestyle=\"--\", alpha=0.2, label=\"True values\")\n",
    "    axs[0, i].plot(xs, nn_values, label=\"Current approx\")\n",
    "    axs[0, i].scatter(dataset.inputs, dataset.outputs, marker=\"x\", label=\"data\")\n",
    "\n",
    "    net_deep = MLPdeep(hidden_dim=20)\n",
    "    train(net_deep, dataset, N_steps, batch_size, lr)\n",
    "\n",
    "    xs = torch.linspace(-1, 1, n_points).reshape(n_points, 1)\n",
    "    nn_values = net_deep(xs).detach().numpy()\n",
    "    true_vals = dataset.true_f(xs)\n",
    "    axs[1, i].plot(xs, true_vals, linestyle=\"--\", alpha=0.2, label=\"True values\")\n",
    "    axs[1, i].plot(xs, nn_values, label=\"Current approx\")\n",
    "    axs[1, i].scatter(dataset.inputs, dataset.outputs, marker=\"x\", label=\"data\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximating a gaussian function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = [2, 4, 32]\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "for d in ds:\n",
    "    dataset = Data(d**3 + 100, type=\"gauss\", input_dim=d)\n",
    "    N_steps = 200\n",
    "    batch_size = 100\n",
    "    lr = 0.1\n",
    "\n",
    "    net_shallow = MLPshallow(hidden_dim=20, input_dim=d)\n",
    "    axs[0].set_title(\"Shallow\")\n",
    "    axs[0].plot(train(net_shallow, dataset, N_steps, batch_size, lr), label=d)\n",
    "    axs[0].legend()\n",
    "\n",
    "    net_deep = MLPdeep(hidden_dim=20, input_dim=d)\n",
    "    axs[1].set_title(\"Deep\")\n",
    "    axs[1].plot(train(net_deep, dataset, N_steps, batch_size, lr), label=d)\n",
    "    plt.legend()\n",
    "plt.show()\n",
    "# xs = torch.linspace(-1, 1, n_points).reshape(n_points, 1)\n",
    "# nn_values = net_deep(xs).detach().numpy()\n",
    "# true_vals = dataset.true_f(xs)\n",
    "# axs[1, i].plot(xs, true_vals, linestyle=\"--\", alpha=0.2, label=\"True values\")\n",
    "# axs[1, i].plot(xs, nn_values, label=\"Current approx\")\n",
    "# axs[1, i].scatter(dataset.inputs, dataset.outputs, marker=\"x\", label=\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not good practice to train networks. For performance, the hyperparameters for each problem and architecture should be tuned separately and you should monitor the state of the network (at least plot the losses during training)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the following code snippet to save your network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp = MLPdeep() # MLPshallow()\n",
    "# PATH = f'./{dataset.type}_{mlp.net_type}.pth'\n",
    "# torch.save(mlp.state_dict(), PATH)\n",
    "\n",
    "# net = MLPdeep() # MLPshallow()\n",
    "# net.load_state_dict(torch.load(PATH))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
