{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Neural Tangent Kernel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "import torchvision\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Today we study attempt to compare the behavior of neural nets and their NTK on some simple example.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# The data\n",
        "\n",
        "\n",
        "To do so we are going to use the FashionMNIST dataset to build a small binary classification task, train some neural nets on those tasks, and build the corresponding NTK. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n = 200 # number of training points that we keep\n",
        "c1, c2 = 3, 6 # subclasses that are kept, make sure that c1 < c2\n",
        "\n",
        "batch_size = 32 # training batch size\n",
        "\n",
        "classes = (\"T-shirt/Top\",\n",
        "        \"Trouser\",\n",
        "        \"Pullover\",\n",
        "        \"Dress\",\n",
        "        \"Coat\", \n",
        "        \"Sandal\", \n",
        "        \"Shirt\",\n",
        "        \"Sneaker\",\n",
        "        \"Bag\",\n",
        "        \"Ankle Boot\"\n",
        ")\n",
        "\n",
        "# Define a sequence of operations that will be performed to all training images before use\n",
        "# the 'ToTensor()' function sets the image in tensor object and puts the values of every pixel between 0 and 1\n",
        "# the 'Normalize' performs the dataset to a given mean and variance\n",
        "transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,))\n",
        "        ])\n",
        "\n",
        "# Creates an object to load the images. We use FashionMNIST as a substitute for MNIST because binary classification on \n",
        "# subclasses of MNIST is too easy. \n",
        "\n",
        "trainset = datasets.FashionMNIST('../data', train=True, download=True, transform=transform)\n",
        "\n",
        "r = torch.arange(len(trainset))\n",
        "\n",
        "# Build a training set that only contains images with classes c1 and c2, with n / 2 images from each label. \n",
        "\n",
        "idxc1 = (torch.as_tensor(trainset.targets) == c1)\n",
        "x1 = np.where(np.cumsum(idxc1) == (n / 2))[0][0]\n",
        "idxc1 = idxc1 & (r <= x1)\n",
        "\n",
        "idxc2 = (torch.as_tensor(trainset.targets) == c2) \n",
        "x2 = np.where(np.cumsum(idxc2) == (n / 2))[0][0]\n",
        "idxc2 = (torch.as_tensor(trainset.targets) == c2) & (r <= x2)\n",
        "\n",
        "idx = idxc1 + idxc2\n",
        "dset_train = torch.utils.data.dataset.Subset(trainset, np.where(idx==1)[0])\n",
        "\n",
        "print(f'Number of training points : {len(dset_train)}')\n",
        "trainloader = torch.utils.data.DataLoader(dset_train, batch_size=batch_size)\n",
        "\n",
        "# Build the corresponding test set\n",
        "\n",
        "testset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
        "idx = torch.as_tensor(testset.targets) == c1\n",
        "idx += torch.as_tensor(testset.targets) == c2\n",
        "dset_test = torch.utils.data.dataset.Subset(testset, np.where(idx==1)[0])\n",
        "testloader = torch.utils.data.DataLoader(dset_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# get batch_size random training images\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "# print labels\n",
        "print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# A neural network\n",
        "\n",
        "Fully connected 1-hidden layer neural network. Flattens the image and treats it as a vector. \n",
        "\n",
        "Use the methods\n",
        "\n",
        "`nn.Flatten`\n",
        "`nn.Linear`\n",
        "\n",
        "`nn.init.xavier_normal_`\n",
        "`nn.init.zeros_`\n",
        "\n",
        "to build the network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MLPshallow(nn.Module): \n",
        "    def __init__(self, hidden_dim=10):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        # YOUR CODE HERE\n",
        "            \n",
        "    def forward(self, x):\n",
        "        # YOUR CODE HERE\n",
        "        return x\n",
        "\n",
        "net = MLPshallow(hidden_dim=1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The training function\n",
        "\n",
        "We train the network with the square loss to predict the class. The labels are normalized to be between $0$ and $1$.\n",
        "\n",
        "i.e. the y-value for images with label `i`  is `float(i/9)`\n",
        "\n",
        "Write the training function. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(net, \n",
        "          trainloader, \n",
        "          N_passes=1, \n",
        "          lr=0.01):\n",
        "    \n",
        "    optimizer = optim.SGD(net.parameters(), lr=lr)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    losses = []\n",
        "    i = 0\n",
        "\n",
        "    for _ in range(N_passes):\n",
        "        for inputs, labels in trainloader: \n",
        "            i += 1\n",
        "            loss = None # Replace with your code\n",
        "            #YOUR CODE HERE\n",
        "\n",
        "            losses.append(loss.detach().numpy())\n",
        "\n",
        "    print(f'Number of gradient steps {i}')\n",
        "\n",
        "    return losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Batch size : ', batch_size)\n",
        "\n",
        "losses = train(net, \n",
        "    trainloader,\n",
        "    N_passes=200,  \n",
        "    lr=0.01)\n",
        "\n",
        "plt.ylim(0, 1.1 * np.max(losses))\n",
        "plt.plot(losses)\n",
        "plt.grid(alpha=.3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Checking the training values\n",
        "\n",
        "Complete the following function to get the prediction of the network on the data. \n",
        "\n",
        "Train the network until you interpolate the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in trainloader:\n",
        "\n",
        "        outputs = net(images)\n",
        "        predicted = #YOUR CODE HERE\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted.squeeze(1) == labels).sum()\n",
        "\n",
        "print(f'Accuracy of the network on the {total} train images: {100 * correct // total} %')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "testloader = torch.utils.data.DataLoader(dset_test, batch_size=8)\n",
        "dataiter = iter(testloader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# print images\n",
        "imshow(torchvision.utils.make_grid(images[:8]))\n",
        "print('GrndTruth: ', ' '.join(f'{classes[labels[j]]:5s}' for j in range(8)))\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = net(images)\n",
        "    predicted = #YOUR CODE HERE\n",
        "\n",
        "    print('Predicted: ', ' '.join(f'{classes[predicted[j]]:5s}' for j in range(8)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in testloader:\n",
        "        outputs = net(images)\n",
        "        predicted = #YOUR CODE HERE\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted.squeeze(1) == labels).sum()\n",
        "\n",
        "print(f'Accuracy of the network on the {total} test images: {100 * correct // total} %')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# The NTK\n",
        "\n",
        "Let us write the corresponding NTK in the infinite width limit. Use the formula seen in class for the NTK kernel to complete the code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NTK:\n",
        "    def __init__(self, dset_train):\n",
        "        self.n = len(dset_train)\n",
        "        self.train_set = dset_train\n",
        "        self.train()\n",
        "        \n",
        "    def k(self, x, xprime):\n",
        "        '''\n",
        "            NTK Kernel for ReLU with one hidden layer. The delta factor is to avoid some annoying rounding for arrccos. \n",
        "        '''\n",
        "        delta = .999999\n",
        "        with torch.no_grad():\n",
        "            v = torch.linalg.norm(x) * torch.linalg.norm(xprime)\n",
        "            u = delta * torch.dot(x, xprime) / v\n",
        "            return # YOUR CODE HERE\n",
        "\n",
        "    def train(self):\n",
        "        ntrainloader = torch.utils.data.DataLoader(self.train_set, batch_size=self.n)\n",
        "        dataiter = iter(ntrainloader)\n",
        "        images, labels = next(dataiter)\n",
        "        print(images.flatten(start_dim=1, end_dim=3).shape)\n",
        "\n",
        "        xis = images.flatten(start_dim=1, end_dim=3)\n",
        "        print(xis.shape)\n",
        "\n",
        "        self.xis = xis\n",
        "\n",
        "        # plt.imshow(xis)\n",
        "        # plt.title('Flattened images')\n",
        "        # plt.show()\n",
        "\n",
        "        H = torch.empty((n, n))\n",
        "\n",
        "        for i in range(n):\n",
        "            for j in range(n):\n",
        "                H[i, j] = self.k(xis[i], xis[j])\n",
        "\n",
        "        plt.title(f'Gram matrix for {n} x {n} inputs')\n",
        "        plt.imshow(H)\n",
        "        plt.colorbar()\n",
        "        plt.show()\n",
        "\n",
        "        eigenvalues = np.linalg.eigvalsh(H)\n",
        "        print(f'Smallest eigenvalue : {eigenvalues[0]}')\n",
        "        # plt.title('Sorted eigenvalues of the Gram matrix')\n",
        "        # plt.plot(eigenvalues)\n",
        "        # plt.grid(alpha=0.3)\n",
        "        # plt.ylim(0, 1.01 * np.max(eigenvalues))\n",
        "        # plt.show()\n",
        "\n",
        "        self.H = H\n",
        "        self.Hinv = torch.linalg.inv(H)\n",
        "        self.V =  self.Hinv @ labels.float() / 9\n",
        "\n",
        "        print(self.V.shape)\n",
        "\n",
        "        # plt.imshow(V)\n",
        "        # plt.colorbar()\n",
        "        # plt.show()\n",
        "\n",
        "    def apply(self, x):\n",
        "        s = 0\n",
        "        for i, xi in enumerate(self.xis):\n",
        "            s += self.k(x, xi) * self.V[i] \n",
        "        return s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ntk = NTK(dset_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ntk_trainloader = torch.utils.data.DataLoader(dset_train, batch_size=1)\n",
        "\n",
        "correct_ntk = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in ntk_trainloader:\n",
        "        x = torch.flatten(images.squeeze(0)) \n",
        "    \n",
        "        output_ntk = ntk.apply(x)\n",
        "        predicted_ntk = c1 + (c2 - c1) * (9 * output_ntk > (c1 + c2) / 2) # c1 if prediction is less than average, \n",
        "        correct_ntk += (predicted_ntk.unsqueeze(0) == labels).numpy()[0]\n",
        "\n",
        "        total += 1\n",
        "\n",
        "print(f'Accuracy of NTK on the {total} train images: {100 * correct_ntk // total} %')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us now compare the outputs of the NTK and of the network. \n",
        "\n",
        "What do you observe? Why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "testloader = torch.utils.data.DataLoader(dset_test)\n",
        "\n",
        "total = 0\n",
        "correct_ntk = 0\n",
        "correct_net = 0\n",
        "agree = 0 \n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in testloader:\n",
        "        x = torch.flatten(images.squeeze(0)) \n",
        "    \n",
        "        output_ntk = ntk.apply(x)\n",
        "        predicted_ntk = c1 + (c2 - c1) * (9 * output_ntk > (c1 + c2) / 2) # c1 if prediction is less than average, \n",
        "        correct_ntk += (predicted_ntk.unsqueeze(0) == labels).numpy()[0]\n",
        "\n",
        "        outputs = net(images)\n",
        "        predicted = c1 + (c2 - c1) * (9 * outputs > (c1 + c2) / 2) # c1 if prediction is less than average, \n",
        "        correct_net += (predicted.squeeze(1) == labels).numpy()[0]\n",
        "\n",
        "        # print(labels.shape)\n",
        "        # print(predicted_ntk.unsqueeze(0).shape, predicted_ntk.unsqueeze(0))\n",
        "        # print(predicted.squeeze(1).shape, predicted.squeeze(1))\n",
        "\n",
        "        agree += (predicted_ntk.unsqueeze(0) == predicted.squeeze(1)).numpy()[0]\n",
        "        total += 1\n",
        "\n",
        "print(f'Accuracy of NTK on the {total} test images: {100 * correct_ntk // total} %')\n",
        "print(f'Accuracy of net on the {total} test images: {100 * correct_net // total} %')\n",
        "\n",
        "print(f'Both methods agree on {100 * agree // total} % of the images')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# The NTK Gram matrix is typically invertible \n",
        "\n",
        "if number of data points is smaller than input dim\n",
        "\n",
        "We plot a ReLU NTK matrix for random data and check that the eigenvalues are typically ok. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n = 30\n",
        "d = 100\n",
        "\n",
        "xis = [torch.randn(d) for _ in range(n)]\n",
        "\n",
        "H = torch.empty((n, n))\n",
        "\n",
        "def k(x, xprime):\n",
        "    with torch.no_grad():\n",
        "        v = torch.linalg.norm(x) * torch.linalg.norm(xprime)\n",
        "        u = .99999 * torch.dot(x, xprime) / v\n",
        "        return v * (u * (torch.pi - torch.arccos(u) + torch.sqrt(1 - u ** 2) )/ (2 * np.pi)\n",
        "                    +  u * (torch.pi - torch.arccos(u)) /  (2 * np.pi))\n",
        "\n",
        "for i in range(n):\n",
        "    for j in range(n):\n",
        "        H[i,j] = k(xis[i], xis[j])\n",
        "\n",
        "\n",
        "plt.title(f'Gram matrix for {n} x {n} inputs of dim {d}')\n",
        "plt.imshow(H)\n",
        "plt.colorbar()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "plt.title('Sorted eigenvalues of the Gram matrix')\n",
        "eigvals = np.linalg.eigvalsh(H)\n",
        "plt.plot(eigvals)\n",
        "plt.grid(alpha=0.3)\n",
        "plt.ylim(0, 1.01 * np.max(eigvals))\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# mlp = MLPdeep() # MLPshallow()\n",
        "# PATH = f'./{dataset.type}_{mlp.net_type}.pth'\n",
        "# torch.save(mlp.state_dict(), PATH)\n",
        "\n",
        "# net = MLPdeep() # MLPshallow()\n",
        "# net.load_state_dict(torch.load(PATH))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#  Overparameterized linear regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "p = 1000\n",
        "n = 100\n",
        "\n",
        "X = np.random.normal(size=(p, n))  / np.sqrt(n) \n",
        "Y = np.random.normal(size=(n, 1))\n",
        "\n",
        "XXt = X @ X.transpose()\n",
        "XY = X @ Y\n",
        "\n",
        "fig, axs = plt.subplots(1, 2)\n",
        "\n",
        "axs[0].set_title(r'$X X^\\top $')\n",
        "axs[0].imshow(XXt)\n",
        "\n",
        "eigvals, P = np.linalg.eigh(XXt)\n",
        "print(rf'Largest eigenvalue of of XX^\\top : {max(eigvals)}')\n",
        "\n",
        "axs[1].set_title('Diagonalized')\n",
        "axs[1].imshow(P.transpose() @ XXt @ P)\n",
        "plt.show()\n",
        "\n",
        "print(np.linalg.matrix_rank(X))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "T = 1000\n",
        "eta = 0.001\n",
        "\n",
        "ws = np.zeros((T, p, 1))\n",
        "w = np.random.normal(size=(p, 1))  / np.sqrt(p)\n",
        "\n",
        "losses = []\n",
        "for t in range(T):\n",
        "    w = w - eta * XXt @ w + eta * XY\n",
        "    ws[t] = w\n",
        "\n",
        "    losses.append(np.linalg.norm(X.transpose() @ w  - Y)**2 / n )\n",
        "\n",
        "ws = ws.squeeze(axis=2)\n",
        "fig, axs = plt.subplots(1, 2, figsize=(16, 4))\n",
        "\n",
        "axs[0].set_title('Loss')\n",
        "axs[0].plot(losses) \n",
        "\n",
        "\n",
        "axs[1].set_title('Relative distance of weights to init')\n",
        "weight_evol = np.linalg.norm(ws - ws[0, :], axis=1) / np.linalg.norm(ws[0, :])\n",
        "axs[1].plot(weight_evol, alpha=.5) \n",
        " \n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### In the right basis, most coordinates of w dont move\n",
        "\n",
        "w only moves in a low dimensional subspace: the image of X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(40, 3))\n",
        "\n",
        "plt.title(r'$|w_{t, i} - w_{0, i}|$')\n",
        "ws_in_base = ws @ P\n",
        "\n",
        "a = ax.imshow(np.abs((ws_in_base - ws_in_base[0]).transpose()))\n",
        "fig.colorbar(a)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
